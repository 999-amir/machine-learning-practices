{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f0a3a2",
   "metadata": {},
   "source": [
    "<div class=\"text-warning\">\n",
    "    <div class=\"text-center\">\n",
    "        <div class=\"h1\">practise 1 - hugging face summarizer</div>\n",
    "        <div class=\"h5\">maktabkhooneh</div>\n",
    "    </div>\n",
    "    <div class=\"h4\">Name: Amir Mohammad Arghavany</div>\n",
    "    <div class=\"h4\">Email: amir.arghavoon@gmail.com</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06753b07",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e0d4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SAM-Tech\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAM-Tech\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from transformers import pipeline\n",
    "import time\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332d085",
   "metadata": {},
   "source": [
    "# get data form internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00bdfccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://medium.com/analytics-vidhya/openai-gpt-3-language-models-are-few-shot-learners-82531b3d3122'\n",
    "element_location = '/html/body/div/div/div[3]/div[2]/div[2]/article/div/div/section/div/div[3]/div/div'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1281a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oper firefox browser as bot\n",
    "driver = webdriver.Firefox()\n",
    "# open website with link\n",
    "driver.get(r'{}'.format(link))\n",
    "time.sleep(3)\n",
    "# find best tag to contain main information\n",
    "article = driver.find_elements(By.XPATH, element_location)\n",
    "# get text from html file\n",
    "article = article[0].text\n",
    "# close firefox browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e0e15",
   "metadata": {},
   "source": [
    "# clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3041831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it easier for GPT-2\n",
    "article_repair = article.replace('?', '').replace('!', '').replace('\\n', ' ').replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f358f76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenAI GPT-3: Language Models are Few-Shot Learners Soheil Tehranipour · Follow Published in Analytics Vidhya · 8 min read · Jun 19, 2020 283 4 GPT-3 Saying a lot at first O penAI recently published a paper describing GPT-3, a deep-learning model for Natural Language Processing, with 175 Billion parameters(), 100x more than the previous version, GPT-2 The model is pre-trained on nearly half a trillion words and achieves SOTA performance on several NLP benchmarks without fine-tuning Did you notice that “Without Fine-Tuning” Compartison: Size of paremeters between models Based on its paper, GPT-3 is an autoregressive language model as opposed to a denoising autoencoder like BERT I decided to write about some of the comparative differences between those two architectures of language models The paper is an investigation into what you can do with giant language models Now this language model is an order of magnitude larger than anyone has ever built a language model OpenAI’s researchers: “In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance,” GPT-3 comes in 8 sizes, ranging from 125M to 175B parameters The largest GPT-3 model is an order of magnitude larger than the previous record holders, T5(11B) and Turing-NLG(17B) The smallest model is ALBERT-Base which is shown in the above chart Language model is all you need So, they seem appropriate for natural language generation One of the things I want to explore here, is mainly an academic endeavor The commercial utility which is something that can help us in industry is not possible with GPT-3 (at least not possible right not) which is only in reach for the very top tech companies like Microsoft,Google or OpenAI But before digging in, let’s Review what happened so far… There’s been such an incredible amount of progress in NLP especially using neural networks which started in 2013 when Tomas Mikolov introduced Word2vec and that was a distributed representation for words which was learned in a self-supervised way based on the context Then we had LSTM type language models and they were learning a sequential representation of tokens through time and after that, there were bidirectional variants A cool thing that happened in about 2018 was Jeremy Howard and Sebastian Ruder released ULMFiT and that introduced transfer learning to Natural Language Processing Word2vec technically is transfer learning but it was transfer learning in the sense of the language model was learning dependencies between a whole bunch of tokens not individual tokens and then of course it could be fine-tuned on a downstream task NLP Timeline after Word2vec Attention is all you need introduce transformers which are these incredible kind of routing machines that they can learn dependencies between any tokens in the input and then it goes through this successive clever routing system which is learned as part of the training process in the original recurrent neural networks The tokens and the input sequence couldn’t directly attend to each other and that meant that those models suffered from catastrophic forgetting and also, they were quite difficult to train because they had vanishing and exploding gradients so transformers came along and it was a different paradigm you could learn a complex hierarchy of relationships directly between the tokens it was no longer this sequential representation where you only knew what happened at one time step backwards it was a real paradigm shift and straight away these language processing models produce SOTA results across the board Generally speaking, there are two architectural patterns that we see with these transformers’ architectures The first is the autoregressive pattern where the model is just predicting the next word and the next word and the next word and the answer from the previous prediction gets fed into the model the next time around 2 The other type of model is what’s called a denoising autoencoder like BERT and RoBERTa and XLNET and what these do is you feed in an input sentence and then you typically add some noise to it and then you say what you expect in the photo below, you can check the size of parameters for each NLP effort before GPT-3 Language models represented by the size of parameters (figure adopted from DistilBERT from huggingface) Now, it’s time to check some famous terms Some Terminologies Transfer Learning is the improvement of learning another task through the transfer of knowledge from a related undertaking task that has already been learned There is a good example of it: “If you know how to play classical piano, then you can learn how to play jazz piano” Transformer (Based on their paper): The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution (For more info click here) Autoregressive: A statistical model is autoregressive if it predicts future values based on past values Language Models learn the probability of word occurrence based on examples of text better said, Language modeling is the task of assigning a probability to sentences in a language Zero-shot learning aims where instances may not have been seen during training and now it’s machine’s turn to do the job One-shot learning is just like the previous one but the difference is the number of seen instances which will be one here Few-shot learning refers to the practice of feeding a learning model with a very small amount of training data, contrary to the normal practice of using a large amount of data (Based on Wikipedia) GPT-3 in a nutshell The OpenAI researchers state that larger models make increasingly efficient use of in-context information As can be seen in the plot below, the steeper “in-context learning curves” for large models show improved ability to learn from contextual information I read somewhere that the whole story of GPT-3 costs “nearly $12 million dollars” I know that it’s not even a penny for Elon Musk but for using GPT-3 in action for practical uses, this is not what the companies like Also, One hypothesis is that GPT-3 has so many parameters (half the number of tokens trained on) that it is starting to act like a memory network As you should know, GPT-2 GPT-2 (a successor to GPT), was trained simply to predict the next word in 40GB of Internet text GPT-2 is a large transformer-based language model with 15 billion parameters, trained on a dataset of 8 million web pages GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data (Based on OpenAI) But, GPT-3 is upgraded with 175 billion parameters, it tailors and escalates the GPT-2 architecture, it also involves adjusted initialization, pre-normalization, and changeable tokenization It reflects substantial performance on various NLP tasks and benchmarks in three distinct shots, ie zero-shot, one-shot and some-shot environments So, these things seem to be appropriate for things like question answering when you need to point to spans in your input sequence for example if you’re doing in-span question answering the auto-regressive models are quite attractive because you can just keep generating data forever For training, the researchers have used a combination of model parallelism within each matrix multiply and model parallelism More information about the training corpus is listed below: GPT-3 was trained on V100 GPU on the part of a high-bandwidth cluster provided by Microsoft Evaluation of GPT-3 is done under 3 conditions: Few-shot learning One-shot learning Zero-shot learning GPT-3 achieved promising results in the zero-shot and one-shot settings, and in the few-shot setting, occasionally surpassed state-of-the-art models The results show that GPT-3 showed strong performance with translation, question-answering, and cloze tasks, as well as with unscrambling words and performing 3-digit arithmetic The researchers claim that GPT-3 can even generate news articles in which human evaluators have difficulty distinguishing from articles written by humans GPT-3 is an incredibly large model, and one cannot expect to build something like this without fancy computational resources However, the researchers assure that these models can be efficient once trained, where even a full GPT-3 model generating 74 pages of content from a trained model can cost only a few cents in energy costs The Code and Test As with GPT-2, OpenAI has not released the trained model or the code, although there is a GitHub repository containing some of the test datasets as well as a collection of text samples generated by the model Conclusion GPT-3 demonstrates that a language model trained on enough data can solve NLP tasks that it has never encountered That is, GPT-3 studies the model as a general solution for many downstream jobs without fine-tuning OpenAI has recently unveiled the latest epitome of its eye-catching text generator, GPT-3 that has 175 billion parameters GPT-3 can execute an amazing bandwidth of natural language processing tasks, even without requiring fine-tuning for a specific task It is capable of performing machine translation, question-answering, reading conceptual tasks, scripting poems, and elementary math'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_repair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4389ef1",
   "metadata": {},
   "source": [
    "# chunk text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99d3ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "sentences = article_repair.split(' ')\n",
    "max_chunk = 500\n",
    "current_chunk = 0 \n",
    "chunks = []\n",
    "for sentence in sentences:\n",
    "    if len(chunks) == current_chunk + 1: \n",
    "        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
    "            chunks[current_chunk].extend(sentence.split(' '))\n",
    "        else:\n",
    "            current_chunk += 1\n",
    "            chunks.append(sentence.split(' '))\n",
    "    else:\n",
    "        print(current_chunk)\n",
    "        chunks.append(sentence.split(' '))\n",
    "\n",
    "for chunk_id in range(len(chunks)):\n",
    "    chunks[chunk_id] = ' '.join(chunks[chunk_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9829bca",
   "metadata": {},
   "source": [
    "# hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6b0d7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baa780b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 120, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n"
     ]
    }
   ],
   "source": [
    "summary = summarizer(chunks, max_length=120, min_length=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1f4730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OpenAI GPT-3: Language Models are Few-Shot Learners soheil Tehranipour writes about what you can do with giant language models . The commercial utility which is something that can help us in industry is not possible .\n",
      " The OpenAI researchers state that larger models make increasingly efficient use of in-context information . The steeper “in-context learning curves” for large models show improved ability to learn from contextual information . I read somewhere that the whole story of GPT-3 costs “nearly $12 million dollars”\n",
      " GPT-3 is upgraded with 175 billion parameters, tailors and escalates the GPT 2 architecture . It reflects substantial performance on various NLP tasks and benchmarks in three distinct shots, ie zero-shot, one-shot and some-shot environments . The researchers claim that GPT 3 can even generate news articles in which human evaluators have difficulty distinguishing from articles written by humans .\n",
      " an amazing bandwidth of natural language processing tasks, even without requiring fine-tuning for a specific task . It is capable of performing machine translation, question-answering, reading conceptual tasks, scripting poems, and elementary math .\n"
     ]
    }
   ],
   "source": [
    "for s in summary:\n",
    "    print(s['summary_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
